/**
 * OpenAI å®¢æˆ·ç«¯å®ç° - å®Œæ•´ç‰ˆ
 * å‚è€ƒ Cherry Studio OpenAIApiClient å®ç°
 * æ”¯æŒæ ‡å‡†çš„ Chat Completions API
 */
import type { Provider } from '../../types/provider';
import type { Chunk } from '../../types/chunk';
import { ChunkType } from '../../types/chunk';
import type {
  SdkModel,
  SdkUsage,
  RequestOptions,
} from '../../types/sdk';
import { BaseApiClient } from '../base';
import type {
  RequestTransformer,
  ResponseChunkTransformer,
  CompletionsContext,
  GenerateImageParams,
  MCPTool,
  MCPToolResponse,
  MCPCallToolResponse,
  Model,
  CompletionsParams,
} from '../base/types';

// ==================== OpenAI Types ====================

interface OpenAIRequestParams {
  model: string;
  messages: OpenAIMessage[];
  stream?: boolean;
  stream_options?: { include_usage: boolean };
  temperature?: number;
  top_p?: number;
  max_tokens?: number;
  max_completion_tokens?: number;
  tools?: OpenAITool[];
  tool_choice?: 'auto' | 'none' | 'required';
  response_format?: { type: 'text' | 'json_object' };
  // æ¨ç†æ¨¡å‹å‚æ•°
  reasoning_effort?: 'low' | 'medium' | 'high';
  reasoning?: { effort?: string; enabled?: boolean };
  // DeepSeek/Qwen æ€è€ƒæ¨¡å¼
  enable_thinking?: boolean;
  thinking?: { type: 'enabled' | 'disabled' | 'auto' };
  thinking_budget?: number;
  // OpenRouter
  include_reasoning?: boolean;
}

interface OpenAIMessage {
  role: 'system' | 'user' | 'assistant' | 'tool' | 'developer';
  content: string | null | OpenAIContentPart[];
  name?: string;
  tool_call_id?: string;
  tool_calls?: OpenAIToolCall[];
}

interface OpenAIContentPart {
  type: 'text' | 'image_url';
  text?: string;
  image_url?: { url: string };
}

interface OpenAITool {
  type: 'function';
  function: {
    name: string;
    description?: string;
    parameters?: Record<string, unknown>;
  };
}

interface OpenAIToolCall {
  id: string;
  type: 'function';
  function: {
    name: string;
    arguments: string;
  };
}

/**
 * OpenAI æµå¼å“åº” Chunk - å®Œæ•´ç±»å‹
 * æ”¯æŒå¤šç§ä¾›åº”å•†çš„æ‰©å±•å­—æ®µ
 */
interface OpenAIStreamChunk {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    // æµå¼å“åº”
    delta?: OpenAIContentSource;
    // éæµå¼å“åº”
    message?: OpenAIContentSource;
    finish_reason: string | null;
  }>;
  usage?: SdkUsage;
  // æ‰©å±•å­—æ®µ
  citations?: any[];
  web_search?: any[];
  search_results?: any[];
  search_info?: { search_results?: any[] };
}

/**
 * å†…å®¹æº - æ”¯æŒ delta å’Œ message ä¸¤ç§æ ¼å¼
 */
interface OpenAIContentSource {
  role?: string;
  content?: string | null;
  // æ¨ç†å†…å®¹ (DeepSeek R1, OpenRouter, etc.)
  reasoning_content?: string;
  reasoning?: string;
  // Doubao æ€è€ƒ
  thinking?: { content?: string };
  // å·¥å…·è°ƒç”¨
  tool_calls?: Array<{
    index: number;
    id?: string;
    type?: string;
    function?: {
      name?: string;
      arguments?: string;
    };
  }>;
  // å›¾ç‰‡ (OpenRouter Gemini)
  images?: Array<{ image_url?: { url: string } }>;
  // æ³¨è§£
  annotations?: any[];
}

// ==================== OpenAI Client ====================

/**
 * OpenAI å®¢æˆ·ç«¯
 */
export class OpenAIClient extends BaseApiClient<
  unknown, // SDK Instance (ä½¿ç”¨ fetch)
  OpenAIRequestParams,
  AsyncIterable<OpenAIStreamChunk>,
  OpenAIStreamChunk,
  OpenAIMessage,
  OpenAIToolCall,
  OpenAITool
> {
  constructor(provider: Provider) {
    super(provider);
  }

  // ==================== SDK Instance ====================

  public getSdkInstance(): unknown {
    // OpenAI ä½¿ç”¨ fetchï¼Œä¸éœ€è¦ SDK å®ä¾‹
    return null;
  }

  // ==================== Core API ====================

  /**
   * æ„å»ºå®Œæ•´çš„ API URL
   * å¤„ç†å„ç§ apiHost æ ¼å¼ï¼š
   * - https://api.openai.com/v1 -> https://api.openai.com/v1/chat/completions
   * - https://xxx.hf.space -> https://xxx.hf.space/v1/chat/completions
   * - https://xxx/v1/ -> https://xxx/v1/chat/completions
   */
  private buildApiUrl(endpoint: string): string {
    let baseUrl = this.getBaseURL();
    
    // ç§»é™¤æœ«å°¾æ–œæ 
    baseUrl = baseUrl.replace(/\/+$/, '');
    
    // å¦‚æœ baseUrl ä¸åŒ…å« /v1ï¼Œæ·»åŠ å®ƒï¼ˆOpenAI å…¼å®¹ API æ ‡å‡†è·¯å¾„ï¼‰
    if (!baseUrl.includes('/v1')) {
      baseUrl = `${baseUrl}/v1`;
    }
    
    return `${baseUrl}${endpoint}`;
  }

  public async createCompletions(
    payload: OpenAIRequestParams,
    options?: RequestOptions
  ): Promise<AsyncIterable<OpenAIStreamChunk>> {
    const url = this.buildApiUrl('/chat/completions');
    
    console.log(`[OpenAIClient] è¯·æ±‚ URL: ${url}`);
    
    // ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ payload ä¸­çš„ stream è®¾ç½®ï¼Œé»˜è®¤ä¸º true
    const streamEnabled = payload.stream !== false;
    
    const response = await fetch(url, {
      method: 'POST',
      headers: this.getDefaultHeaders(),
      body: JSON.stringify({ ...payload, stream: streamEnabled }),
      signal: options?.signal,
    });

    if (!response.ok) {
      const error = await response.text();
      throw new Error(`OpenAI API Error: ${response.status} ${error}`);
    }

    // ğŸ”§ éæµå¼å“åº”ï¼šè§£æ JSON å¹¶è½¬æ¢ä¸ºå•ä¸ª chunk
    if (!streamEnabled) {
      const data = await response.json();
      console.log(`[OpenAIClient] éæµå¼å“åº”:`, data);
      
      // å°†éæµå¼å“åº”åŒ…è£…ä¸ºå¼‚æ­¥è¿­ä»£å™¨
      return (async function* () {
        yield data as OpenAIStreamChunk;
      })();
    }

    return this.parseSSEStream(response);
  }

  public async listModels(): Promise<SdkModel[]> {
    const url = this.buildApiUrl('/models');
    
    const response = await fetch(url, {
      method: 'GET',
      headers: this.getDefaultHeaders(),
    });

    if (!response.ok) {
      throw new Error(`Failed to list models: ${response.status}`);
    }

    const data = await response.json();
    return (data.data || []).map((m: any) => ({
      id: m.id,
      object: m.object,
      created: m.created,
      owned_by: m.owned_by,
    }));
  }

  public async getEmbeddingDimensions(_model?: Model): Promise<number> {
    return 1536; // OpenAI é»˜è®¤
  }

  public async generateImage(params: GenerateImageParams): Promise<string[]> {
    const url = `${this.getBaseURL()}/images/generations`;
    
    const response = await fetch(url, {
      method: 'POST',
      headers: this.getDefaultHeaders(),
      body: JSON.stringify({
        model: params.model || 'dall-e-3',
        prompt: params.prompt,
        n: params.n || 1,
        size: params.size || '1024x1024',
        quality: params.quality || 'standard',
      }),
    });

    if (!response.ok) {
      throw new Error(`Image generation failed: ${response.status}`);
    }

    const data = await response.json();
    return (data.data || []).map((img: any) => img.url).filter(Boolean);
  }

  // ==================== Transformers ====================

  public getRequestTransformer(): RequestTransformer<OpenAIRequestParams, OpenAIMessage> {
    return new OpenAIRequestTransformer(this);
  }

  public getResponseChunkTransformer(ctx: CompletionsContext): ResponseChunkTransformer<OpenAIStreamChunk> {
    return new OpenAIResponseTransformer(ctx);
  }

  // ==================== Tool Conversion ====================

  public convertMcpToolsToSdkTools(mcpTools: MCPTool[]): OpenAITool[] {
    return mcpTools.map(tool => ({
      type: 'function' as const,
      function: {
        name: this.sanitizeToolName(tool.id || tool.name),
        description: tool.description || '',
        parameters: (tool.inputSchema as Record<string, unknown>) || { type: 'object', properties: {} },
      },
    }));
  }

  public convertSdkToolCallToMcp(
    toolCall: OpenAIToolCall,
    mcpTools: MCPTool[]
  ): MCPTool | undefined {
    return mcpTools.find(t =>
      this.sanitizeToolName(t.id || t.name) === toolCall.function.name
    );
  }

  public convertSdkToolCallToMcpToolResponse(
    toolCall: OpenAIToolCall,
    mcpTool: MCPTool
  ): MCPToolResponse {
    return {
      id: toolCall.id,
      toolCallId: toolCall.id,
      tool: mcpTool,
      arguments: JSON.parse(toolCall.function.arguments || '{}'),
      status: 'pending',
    };
  }

  public convertMcpToolResponseToSdkMessageParam(
    mcpToolResponse: MCPToolResponse,
    resp: MCPCallToolResponse,
    _model: Model
  ): OpenAIMessage | undefined {
    return {
      role: 'tool',
      tool_call_id: mcpToolResponse.toolCallId || mcpToolResponse.id,
      content: resp.isError ? `Error: ${this.extractContent(resp)}` : this.extractContent(resp),
    };
  }

  // ==================== Message Handling ====================

  public buildSdkMessages(
    currentReqMessages: OpenAIMessage[],
    _output: unknown,
    toolResults: OpenAIMessage[],
    toolCalls?: OpenAIToolCall[]
  ): OpenAIMessage[] {
    const messages = [...currentReqMessages];

    if (toolCalls && toolCalls.length > 0) {
      messages.push({
        role: 'assistant',
        content: null,
        tool_calls: toolCalls,
      });
    }

    messages.push(...toolResults);
    return messages;
  }

  public extractMessagesFromSdkPayload(sdkPayload: OpenAIRequestParams): OpenAIMessage[] {
    return sdkPayload.messages;
  }

  public estimateMessageTokens(message: OpenAIMessage): number {
    const content = typeof message.content === 'string'
      ? message.content
      : JSON.stringify(message.content);
    return Math.ceil((content?.length || 0) / 4);
  }

  // ==================== Helper Methods ====================

  private async *parseSSEStream(response: Response): AsyncIterable<OpenAIStreamChunk> {
    const reader = response.body?.getReader();
    if (!reader) {
      throw new Error('No response body');
    }

    const decoder = new TextDecoder();
    let buffer = '';

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6).trim();
            if (data === '[DONE]') {
              return;
            }
            try {
              yield JSON.parse(data);
            } catch {
              // å¿½ç•¥è§£æé”™è¯¯
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }
  }

  private extractContent(resp: MCPCallToolResponse): string {
    return resp.content
      .map(c => c.text || '')
      .filter(Boolean)
      .join('\n');
  }

  public getClientCompatibilityType(_model?: Model): string[] {
    return ['OpenAIClient', 'OpenAIAPIClient'];
  }
}

// ==================== Request Transformer ====================

class OpenAIRequestTransformer implements RequestTransformer<OpenAIRequestParams, OpenAIMessage> {
  constructor(private client: OpenAIClient) {}

  transform(params: CompletionsParams): OpenAIRequestParams {
    const { messages, assistant, mcpTools } = params;
    const model = assistant?.model;

    // è½¬æ¢æ¶ˆæ¯
    const sdkMessages: OpenAIMessage[] = [];

    // æ·»åŠ ç³»ç»Ÿæç¤ºè¯
    if (assistant?.prompt) {
      sdkMessages.push({
        role: 'system',
        content: assistant.prompt,
      });
    }

    // è½¬æ¢ç”¨æˆ·æ¶ˆæ¯
    for (const msg of messages) {
      sdkMessages.push(this.transformMessage(msg));
    }

    // æ„å»ºè¯·æ±‚å‚æ•°
    const request: OpenAIRequestParams = {
      model: model?.id || 'gpt-3.5-turbo',
      messages: sdkMessages,
      stream: true,
      temperature: (this.client as any).getTemperature(assistant, model),
      top_p: (this.client as any).getTopP(assistant, model),
      max_tokens: (this.client as any).getMaxTokens(assistant, model),
    };

    // æ·»åŠ å·¥å…·
    if (mcpTools && mcpTools.length > 0) {
      const { tools } = this.client.setupToolsConfig({
        mcpTools,
        model: model || { id: '', name: '', provider: '' },
        enableToolUse: params.enableToolUse,
      });
      if (tools.length > 0) {
        request.tools = tools;
        request.tool_choice = 'auto';
      }
    }

    return request;
  }

  transformMessage(message: any): OpenAIMessage {
    return {
      role: message.role === 'system' ? 'system' : message.role === 'assistant' ? 'assistant' : 'user',
      content: typeof message.content === 'string' ? message.content : message.content || '',
    };
  }
}

// ==================== Response Transformer ====================

/**
 * OpenAI å“åº”è½¬æ¢å™¨ - å®Œæ•´ç‰ˆ
 * å‚è€ƒ Cherry Studio getResponseChunkTransformer å®ç°
 */
class OpenAIResponseTransformer implements ResponseChunkTransformer<OpenAIStreamChunk> {
  private toolCallsBuffer: Map<number, { id: string; name: string; arguments: string }> = new Map();
  private isThinking = false;
  private isAccumulatingText = false;
  private hasFinishReason = false;
  private isFinished = false;
  private lastUsage: SdkUsage | null = null;

  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  constructor(_ctx: CompletionsContext) {
    // _ctx ä¿ç•™ç”¨äºåç»­æ‰©å±•ï¼ˆå¦‚ provider ç‰¹å®šå¤„ç†ï¼‰
  }

  /**
   * è½¬æ¢å•ä¸ª chunk ä¸ºæ ‡å‡† Chunk æ•°ç»„
   * å‚è€ƒ Cherry Studio çš„ transform å®ç°
   */
  transform(chunk: OpenAIStreamChunk): Chunk[] {
    const chunks: Chunk[] = [];

    // æ›´æ–° usage ä¿¡æ¯
    if (chunk.usage) {
      this.lastUsage = {
        prompt_tokens: chunk.usage.prompt_tokens || 0,
        completion_tokens: chunk.usage.completion_tokens || 0,
        total_tokens: chunk.usage.total_tokens || 0,
      };
    }

    // å¦‚æœå·²ç»ç»“æŸï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å‘é€å®Œæˆä¿¡å·
    if (this.hasFinishReason && !this.isFinished) {
      chunks.push(...this.emitCompletionSignals());
      return chunks;
    }

    // å¤„ç† choices
    if (chunk.choices && chunk.choices.length > 0) {
      for (const choice of chunk.choices) {
        if (!choice) continue;

        // è·å–å†…å®¹æºï¼ˆæ”¯æŒ delta å’Œ message ä¸¤ç§æ ¼å¼ï¼‰
        let contentSource: OpenAIContentSource | null = null;
        
        if (choice.delta && Object.keys(choice.delta).length > 0) {
          // æµå¼å“åº”ï¼šæ£€æŸ¥ delta æ˜¯å¦æœ‰å®é™…å†…å®¹
          const delta = choice.delta;
          const hasContent = delta.content || 
            delta.reasoning_content || 
            delta.reasoning || 
            delta.thinking?.content ||
            (delta.tool_calls && delta.tool_calls.length > 0);
          
          if (hasContent || delta.role) {
            contentSource = delta;
          }
        } else if (choice.message) {
          // éæµå¼å“åº”
          contentSource = choice.message;
        }

        // å¦‚æœæ²¡æœ‰å†…å®¹æºï¼Œæ£€æŸ¥ finish_reason
        if (!contentSource) {
          if (choice.finish_reason) {
            this.hasFinishReason = true;
            if (this.lastUsage) {
              chunks.push(...this.emitCompletionSignals());
            }
          }
          continue;
        }

        // === å¤„ç†æ¨ç†/æ€è€ƒå†…å®¹ ===
        const reasoningText = 
          contentSource.reasoning_content || 
          contentSource.reasoning || 
          contentSource.thinking?.content;
        
        if (reasoningText) {
          this.isThinking = true;
          chunks.push({
            type: ChunkType.THINKING_DELTA,
            text: reasoningText,
          });
        } else if (this.isThinking) {
          // æ€è€ƒç»“æŸ
          chunks.push({ type: ChunkType.THINKING_COMPLETE, text: '' });
          this.isThinking = false;
        }

        // === å¤„ç†æ–‡æœ¬å†…å®¹ ===
        if (contentSource.content) {
          this.isAccumulatingText = true;
          chunks.push({
            type: ChunkType.TEXT_DELTA,
            text: contentSource.content,
          });
        } else if (this.isAccumulatingText && !contentSource.tool_calls) {
          this.isAccumulatingText = false;
        }

        // === å¤„ç†å›¾ç‰‡å†…å®¹ (OpenRouter Gemini) ===
        if (contentSource.images && Array.isArray(contentSource.images)) {
          chunks.push({ type: ChunkType.IMAGE_CREATED });
          chunks.push({
            type: ChunkType.IMAGE_COMPLETE,
            image: {
              type: 'base64',
              images: contentSource.images.map(img => img.image_url?.url || ''),
            },
          });
        }

        // === å¤„ç†å·¥å…·è°ƒç”¨ ===
        if (contentSource.tool_calls) {
          for (const toolCall of contentSource.tool_calls) {
            if ('index' in toolCall) {
              const { id, index } = toolCall;
              const func = toolCall.function;
              
              if (func?.name) {
                // æ–°å·¥å…·è°ƒç”¨
                const toolCallObject = {
                  id: id || '',
                  name: func.name,
                  arguments: func.arguments || '',
                };
                
                if (index === -1) {
                  this.toolCallsBuffer.set(this.toolCallsBuffer.size, toolCallObject);
                } else {
                  this.toolCallsBuffer.set(index, toolCallObject);
                }
              } else if (func?.arguments) {
                // è¿½åŠ å‚æ•°
                const existing = this.toolCallsBuffer.get(index);
                if (existing) {
                  existing.arguments += func.arguments;
                }
              }
            }
          }
        }

        // === å¤„ç† finish_reason ===
        if (choice.finish_reason) {
          this.hasFinishReason = true;
          if (this.lastUsage) {
            chunks.push(...this.emitCompletionSignals());
          }
        }
      }
    }

    return chunks;
  }

  /**
   * å‘é€å®Œæˆä¿¡å·
   */
  private emitCompletionSignals(): Chunk[] {
    if (this.isFinished) return [];

    const chunks: Chunk[] = [];

    // å‘é€å·¥å…·è°ƒç”¨å®Œæˆ
    if (this.toolCallsBuffer.size > 0) {
      const toolCalls = Array.from(this.toolCallsBuffer.values());
      chunks.push({
        type: ChunkType.MCP_TOOL_COMPLETE,
        responses: toolCalls.map(tc => ({
          id: tc.id,
          name: tc.name,
          arguments: this.parseArguments(tc.arguments),
        })),
      } as any);
    }

    // å‘é€å“åº”å®Œæˆï¼ˆåŒ…å« usage ä¿¡æ¯ï¼‰
    chunks.push({
      type: ChunkType.LLM_RESPONSE_COMPLETE,
      // Response æ‰©å±•ï¼šæ·»åŠ  usage ä¿¡æ¯
      response: {
        id: 'completion',
        content: '',
      },
      // é¢å¤–å­˜å‚¨ usage
      usage: this.lastUsage || {
        prompt_tokens: 0,
        completion_tokens: 0,
        total_tokens: 0,
      },
    } as any);

    this.isFinished = true;
    return chunks;
  }

  /**
   * å®‰å…¨è§£æ JSON å‚æ•°
   */
  private parseArguments(args: string): Record<string, unknown> {
    try {
      return JSON.parse(args || '{}');
    } catch {
      return { raw: args };
    }
  }
}

export { OpenAIRequestTransformer, OpenAIResponseTransformer };
