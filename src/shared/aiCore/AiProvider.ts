/**
 * AiProvider - AI æä¾›è€…æ ¸å¿ƒç±»
 * å®Œå…¨å‚è€ƒ Cherry Studio aiCore/legacy/index.ts å®ç°
 * 
 * èŒè´£ï¼š
 * 1. æ ¹æ® Provider åˆ›å»ºå¯¹åº”çš„ ApiClient
 * 2. æ„å»ºå’Œç®¡ç†ä¸­é—´ä»¶é“¾
 * 3. æ‰§è¡Œ completions è¯·æ±‚
 * 4. å¤„ç†æµå¼å“åº”
 */

import { ApiClientFactory, initializeDefaultClients } from './clients';
import type { BaseApiClient } from './clients/base';
import type { Provider } from './types/provider';
import type { Chunk } from './types/chunk';
import { ChunkType } from './types/chunk';
import type { SdkModel } from './types/sdk';
import type { Model } from '../types';

// å¯¼å…¥ç°æœ‰çš„ MCP æç¤ºè¯æ„å»ºå‡½æ•°
import { buildSystemPrompt } from '../utils/mcpPrompt';
// å¯¼å…¥ MCP å·¥å…·è°ƒç”¨ç›¸å…³å‡½æ•°
import { parseToolUse, parseAndCallTools, hasToolUseTags } from '../utils/mcpToolParser';

// ==================== Types ====================

/**
 * Completions è¯·æ±‚å‚æ•°
 */
export interface CompletionsParams {
  /** è°ƒç”¨ç±»å‹ */
  callType: 'chat' | 'check' | 'translate' | 'summary' | 'generate' | 'search' | 'test';
  /** æ¶ˆæ¯å†…å®¹ï¼ˆå¯ä»¥æ˜¯æ¶ˆæ¯æ•°ç»„æˆ–å­—ç¬¦ä¸²ï¼‰*/
  messages: Message[] | string;
  /** åŠ©æ‰‹é…ç½® */
  assistant: Assistant;
  /** æ˜¯å¦æµå¼è¾“å‡º */
  streamOutput?: boolean;
  /** ä¸»é¢˜ID */
  topicId?: string;
  /** MCPå·¥å…·åˆ—è¡¨ */
  mcpTools?: MCPTool[];
  /** MCP æ¨¡å¼ï¼šprompt=æç¤ºè¯æ³¨å…¥, function=å‡½æ•°è°ƒç”¨ */
  mcpMode?: 'prompt' | 'function';
  /** æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢ */
  enableWebSearch?: boolean;
  /** æ˜¯å¦å¯ç”¨å›¾ç‰‡ç”Ÿæˆ */
  enableGenerateImage?: boolean;
  /** æœ€å¤§ tokens */
  maxTokens?: number;
  /** æ˜¯å¦åº”è¯¥æŠ›å‡ºé”™è¯¯ */
  shouldThrow?: boolean;
  /** Chunk å›è°ƒ */
  onChunk?: (chunk: Chunk) => void;
  /** ä¸­æ–­ä¿¡å· */
  abortSignal?: AbortSignal;
}

/**
 * Completions ç»“æœ
 */
export interface CompletionsResult {
  /** è·å–æ–‡æœ¬å†…å®¹ */
  getText: () => string;
  /** è·å–æ¨ç†å†…å®¹ */
  getReasoning: () => string | undefined;
  /** è·å–åŸå§‹è¾“å‡º */
  rawOutput?: unknown;
  /** ä½¿ç”¨ç»Ÿè®¡ */
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

/**
 * æ¶ˆæ¯ç±»å‹
 */
export interface Message {
  id?: string;
  role: 'user' | 'assistant' | 'system';
  content: string;
  topicId?: string;
  [key: string]: unknown;
}

/**
 * åŠ©æ‰‹é…ç½®
 */
export interface Assistant {
  id: string;
  name?: string;
  prompt?: string;
  model?: Model;
  settings?: {
    temperature?: number;
    topP?: number;
    maxTokens?: number;
    streamOutput?: boolean;
    reasoning_effort?: 'low' | 'medium' | 'high';
    [key: string]: unknown;
  };
  mcpServers?: { id: string }[];
  [key: string]: unknown;
}

/**
 * MCP å·¥å…·
 */
export interface MCPTool {
  id?: string;
  name: string;
  description?: string;
  inputSchema?: unknown;
  serverId?: string;
  [key: string]: unknown;
}

/**
 * å›¾ç‰‡ç”Ÿæˆå‚æ•°
 */
export interface GenerateImageParams {
  model: string;
  prompt: string;
  negativePrompt?: string;
  imageSize?: string;
  batchSize?: number;
  seed?: string;
  numInferenceSteps?: number;
  guidanceScale?: number;
  signal?: AbortSignal;
  promptEnhancement?: boolean;
}

// ==================== AiProvider Class ====================

/**
 * AI æä¾›è€…ç±»
 * æ ¸å¿ƒå…¥å£ï¼Œè´Ÿè´£åˆ›å»ºå®¢æˆ·ç«¯ã€æ„å»ºä¸­é—´ä»¶ã€æ‰§è¡Œè¯·æ±‚
 */
export default class AiProvider {
  private apiClient: BaseApiClient;
  private provider: Provider;
  private initialized = false;

  constructor(provider: Provider) {
    this.provider = provider;
    // å»¶è¿Ÿåˆå§‹åŒ–å®¢æˆ·ç«¯
    this.apiClient = null as unknown as BaseApiClient;
  }

  /**
   * ç¡®ä¿å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
   */
  private async ensureInitialized(): Promise<void> {
    if (this.initialized) return;
    
    await initializeDefaultClients();
    this.apiClient = ApiClientFactory.create(this.provider);
    this.initialized = true;
    
    console.log(`[AiProvider] åˆå§‹åŒ–å®Œæˆ - Provider: ${this.provider.id}, Type: ${this.provider.type}`);
  }

  /**
   * æ‰§è¡Œ Completions è¯·æ±‚
   * æ ¸å¿ƒæ–¹æ³•ï¼Œå¤„ç†æµå¼/éæµå¼å“åº”
   */
  public async completions(
    params: CompletionsParams,
    options?: { signal?: AbortSignal; timeout?: number }
  ): Promise<CompletionsResult> {
    await this.ensureInitialized();

    const {
      messages,
      assistant,
      streamOutput = true,
      onChunk,
      mcpTools,
    } = params;

    const model = assistant.model;
    if (!model) {
      throw new Error('Model is required');
    }

    console.log(`[AiProvider] completions - Model: ${model.id}, Stream: ${streamOutput}`);

    // ç´¯ç§¯çš„ç»“æœ
    let accumulatedText = '';
    let accumulatedReasoning = '';
    let usage: CompletionsResult['usage'];
    
    // ğŸ”§ æ€è€ƒæ—¶é—´è·Ÿè¸ª
    let thinkingStartTime = 0;
    let hasStartedThinking = false;
    // ğŸ”§ æ–‡æœ¬å¼€å§‹æ ‡å¿—ï¼ˆå‚è€ƒ Cherry Studioï¼‰
    let hasStartedText = false;

    try {
      // 1. è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼ˆä¼ å…¥ mcpTools å’Œ mcpMode æ”¯æŒæç¤ºè¯æ³¨å…¥ï¼‰
      const sdkMessages = this.transformMessages(messages, assistant, mcpTools, params.mcpMode);

      // 2. æ„å»º SDK è¯·æ±‚å‚æ•°
      const transformer = this.apiClient.getRequestTransformer();
      const sdkPayload = transformer.transform({
        messages: sdkMessages.map((m, i) => ({
          id: m.id || `msg-${i}`,
          role: m.role,
          content: m.content,
        })),
        assistant,
        mcpTools: mcpTools?.map(t => ({
          ...t,
          serverName: t.serverId || 'unknown',
        })) as any,
        enableToolUse: !!mcpTools?.length,
      });
      
      // ğŸ”§ è®¾ç½®æµå¼è¾“å‡º
      (sdkPayload as any).stream = streamOutput;

      // 3. å‘é€ LLM_RESPONSE_CREATED
      if (onChunk) {
        await onChunk({ type: ChunkType.LLM_RESPONSE_CREATED });
      }

      // 4. æ‰§è¡Œè¯·æ±‚
      const rawStream = await this.apiClient.createCompletions(
        sdkPayload as any,
        { signal: options?.signal || params.abortSignal }
      );

      // 5. å¤„ç†æµå¼å“åº”
      // ğŸ”§ å‚ç…§ Cherry Studio ThinkChunkMiddleware çš„è®¾è®¡ï¼š
      // - THINKING_DELTA.text æ˜¯ç´¯ç§¯çš„å®Œæ•´å†…å®¹ï¼Œä¸æ˜¯å¢é‡
      // - æ”¶åˆ°éæ€è€ƒ chunk æ—¶æ‰å‘é€ THINKING_COMPLETE
      for await (const rawChunk of rawStream as AsyncIterable<any>) {
        // ğŸ”§ è°ƒè¯•ï¼šæ‰“å°åŸå§‹ chunk
        if (!streamOutput) {
          console.log(`[AiProvider] éæµå¼ rawChunk:`, JSON.stringify(rawChunk).substring(0, 500));
        }
        
        // è§£æ chunkï¼Œä¸ä¼  onChunkï¼ˆç”±æˆ‘ä»¬ç»Ÿä¸€å¤„ç†ï¼‰
        const result = this.processChunk(rawChunk);
        
        // ğŸ”§ è°ƒè¯•ï¼šæ‰“å°è§£æç»“æœ
        if (!streamOutput) {
          console.log(`[AiProvider] éæµå¼è§£æç»“æœ:`, { text: result.text?.substring(0, 100), reasoning: result.reasoning?.substring(0, 100) });
        }
        
        // å¤„ç†æ€è€ƒå†…å®¹
        if (result.reasoning) {
          // ç¬¬ä¸€æ¬¡æ¥æ”¶åˆ°æ€è€ƒå†…å®¹æ—¶è®°å½•å¼€å§‹æ—¶é—´
          if (!hasStartedThinking) {
            hasStartedThinking = true;
            thinkingStartTime = Date.now();
            // ğŸ”§ åªåœ¨æµå¼æ¨¡å¼ä¸‹å‘é€ THINKING_STARTï¼ˆéæµå¼ä¸éœ€è¦å¤šè½®é‡ç½®ï¼‰
            console.log('[AiProvider] å‡†å¤‡å‘é€ THINKING_START', { onChunk: !!onChunk, streamOutput });
            if (onChunk && streamOutput) {
              await onChunk({ type: ChunkType.THINKING_START } as Chunk);
              console.log('[AiProvider] THINKING_START å·²å‘é€');
            }
          }
          
          // ç´¯ç§¯æ€è€ƒå†…å®¹
          accumulatedReasoning += result.reasoning;
          
          // ğŸ”§ å…³é”®ï¼šå‘é€çš„ text æ˜¯ç´¯ç§¯å†…å®¹ï¼Œä¸æ˜¯å¢é‡
          // éæµå¼æ¨¡å¼ç›´æ¥å‘é€ THINKING_COMPLETEï¼ˆå› ä¸ºä¸€æ¬¡æ€§å®Œæˆï¼‰
          if (onChunk) {
            if (streamOutput) {
              await onChunk({
                type: ChunkType.THINKING_DELTA,
                text: accumulatedReasoning,
                thinking_millsec: Date.now() - thinkingStartTime,
              } as Chunk);
            }
            // éæµå¼æ¨¡å¼çš„æ€è€ƒå†…å®¹åœ¨å¤„ç†æ–‡æœ¬æ—¶ä¸€èµ·å‘é€ THINKING_COMPLETE
          }
        }
        
        // å¤„ç†æ–‡æœ¬å†…å®¹
        if (result.text) {
          // ğŸ”§ æ”¶åˆ°æ–‡æœ¬æ—¶ï¼Œå¦‚æœä¹‹å‰æœ‰æ€è€ƒå†…å®¹ï¼Œå…ˆå‘é€ THINKING_COMPLETE
          if (hasStartedThinking && thinkingStartTime > 0) {
            if (onChunk) {
              await onChunk({
                type: ChunkType.THINKING_COMPLETE,
                text: accumulatedReasoning,
                thinking_millsec: Date.now() - thinkingStartTime,
              } as Chunk);
            }
            // é‡ç½®æ€è€ƒçŠ¶æ€
            hasStartedThinking = false;
            thinkingStartTime = 0;
          }
          
          // ğŸ”§ å‚è€ƒ Cherry Studioï¼šç¬¬ä¸€æ¬¡å‘é€æ–‡æœ¬å‰ï¼Œå…ˆå‘é€ TEXT_START
          if (!hasStartedText && streamOutput) {
            hasStartedText = true;
            console.log('[AiProvider] å‡†å¤‡å‘é€ TEXT_START', { onChunk: !!onChunk, streamOutput });
            if (onChunk) {
              await onChunk({ type: ChunkType.TEXT_START } as Chunk);
              console.log('[AiProvider] TEXT_START å·²å‘é€');
            }
          }
          
          accumulatedText += result.text;
          if (onChunk) {
            // ğŸ”§ éæµå¼æ¨¡å¼å‘é€ TEXT_COMPLETEï¼Œæµå¼æ¨¡å¼å‘é€ TEXT_DELTA
            // å‚è€ƒ Cherry Studioï¼šå‘é€ç´¯ç§¯çš„æ–‡æœ¬ï¼Œä¸æ˜¯å¢é‡
            await onChunk({
              type: streamOutput ? ChunkType.TEXT_DELTA : ChunkType.TEXT_COMPLETE,
              text: accumulatedText,
            } as Chunk);
          }
        }
        
        if (result.usage) {
          usage = result.usage;
        }
      }
      
      // ğŸ”§ æµç»“æŸåï¼Œå¦‚æœè¿˜æœ‰æœªå®Œæˆçš„æ€è€ƒå†…å®¹ï¼Œå‘é€ THINKING_COMPLETE
      if (hasStartedThinking && thinkingStartTime > 0 && accumulatedReasoning) {
        if (onChunk) {
          await onChunk({
            type: ChunkType.THINKING_COMPLETE,
            text: accumulatedReasoning,
            thinking_millsec: Date.now() - thinkingStartTime,
          } as Chunk);
        }
        hasStartedThinking = false;
        thinkingStartTime = 0;
      }

      // 6. æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆæç¤ºè¯æ³¨å…¥æ¨¡å¼çš„å¤šè½®å·¥å…·è°ƒç”¨ï¼‰
      if (params.mcpMode === 'prompt' && mcpTools && mcpTools.length > 0) {
        const hasTools = hasToolUseTags(accumulatedText, mcpTools as any);
        
        if (hasTools) {
          console.log(`[AiProvider] ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œå¼€å§‹æ‰§è¡Œ...`);
          
          // è§£æå·¥å…·è°ƒç”¨
          const toolResponses = parseToolUse(accumulatedText, mcpTools as any);
          
          if (toolResponses.length > 0) {
            console.log(`[AiProvider] è§£æå‡º ${toolResponses.length} ä¸ªå·¥å…·è°ƒç”¨`);
            
            // æ‰§è¡Œå·¥å…·è°ƒç”¨
            const toolResults = await parseAndCallTools(toolResponses, mcpTools as any, onChunk);
            
            // æ ¼å¼åŒ–å·¥å…·ç»“æœ
            const toolResultsText = toolResults.map((result, index) => {
              const toolResponse = toolResponses[index];
              if (result.isError) {
                return `<tool_use_result>\n  <name>${toolResponse.tool.name}</name>\n  <error>${result.content.map(c => c.text).join('\n')}</error>\n</tool_use_result>`;
              } else {
                return `<tool_use_result>\n  <name>${toolResponse.tool.name}</name>\n  <result>${JSON.stringify(result.content)}</result>\n</tool_use_result>`;
              }
            }).join('\n\n');
            
            console.log(`[AiProvider] ğŸ”„ é€’å½’è°ƒç”¨ LLMï¼Œä¼ é€’å·¥å…·ç»“æœ...`);
            
            // ğŸ”§ å‘é€å½“å‰è½®å“åº”å®Œæˆä¿¡å·
            if (onChunk) {
              onChunk({
                type: ChunkType.LLM_RESPONSE_COMPLETE,
                response: { id: 'tool-call-response', content: accumulatedText },
              } as Chunk);
            }
            
            // æ„å»ºé€’å½’è°ƒç”¨çš„æ¶ˆæ¯ï¼ˆä¸åŒ…å«ç³»ç»Ÿæç¤ºè¯ï¼Œå› ä¸ºé€’å½’æ—¶ä¼šé‡æ–°æ„å»ºï¼‰
            const originalMessages = Array.isArray(messages) ? messages : [{ role: 'user' as const, content: messages }];
            const newMessages: Message[] = [
              ...originalMessages,
              { role: 'assistant', content: accumulatedText },
              { role: 'user', content: toolResultsText }
            ];
            
            // é€’å½’è°ƒç”¨ï¼ˆæœ€å¤šé€’å½’ 5 æ¬¡é˜²æ­¢æ— é™å¾ªç¯ï¼‰
            const recursionDepth = (params as any)._recursionDepth || 0;
            if (recursionDepth < 5) {
              // ğŸ”§ å‘é€æ–°ä¸€è½®å“åº”å¼€å§‹ä¿¡å·
              if (onChunk) {
                await onChunk({ type: ChunkType.LLM_RESPONSE_CREATED } as Chunk);
              }
              
              const recursiveResult = await this.completions({
                ...params,
                messages: newMessages,
                _recursionDepth: recursionDepth + 1,
              } as any, options);
              
              // åˆå¹¶ç»“æœï¼ˆä¸å†ç®€å•æ‹¼æ¥ï¼Œå› ä¸ºæ¯è½®éƒ½æ˜¯ç‹¬ç«‹çš„ï¼‰
              const finalText = recursiveResult.getText();
              const finalReasoning = recursiveResult.getReasoning();
              
              return {
                getText: () => finalText,
                getReasoning: () => finalReasoning || undefined,
                usage,
              };
            } else {
              console.warn(`[AiProvider] âš ï¸ è¾¾åˆ°æœ€å¤§é€’å½’æ·±åº¦ï¼Œåœæ­¢å·¥å…·è°ƒç”¨`);
            }
          }
        }
      }

      // 7. å‘é€å®Œæˆä¿¡å·
      // ğŸ”§ å‚è€ƒ Cherry Studioï¼šå…ˆå‘é€ TEXT_COMPLETEï¼Œå†å‘é€ LLM_RESPONSE_COMPLETE
      if (onChunk && accumulatedText && streamOutput) {
        await onChunk({
          type: ChunkType.TEXT_COMPLETE,
          text: accumulatedText,
        } as Chunk);
      }
      
      if (onChunk) {
        await onChunk({
          type: ChunkType.LLM_RESPONSE_COMPLETE,
          response: { id: 'completion', content: accumulatedText },
        } as Chunk);
      }

      console.log(`[AiProvider] completions å®Œæˆ - æ–‡æœ¬é•¿åº¦: ${accumulatedText.length}, æ¨ç†é•¿åº¦: ${accumulatedReasoning.length}`);

      return {
        getText: () => accumulatedText,
        getReasoning: () => accumulatedReasoning || undefined,
        usage,
      };
    } catch (error) {
      console.error('[AiProvider] completions é”™è¯¯:', error);
      
      // å‘é€é”™è¯¯ chunk
      if (onChunk) {
        await onChunk({
          type: ChunkType.ERROR,
          error: {
            message: error instanceof Error ? error.message : String(error),
          },
        });
      }

      if (params.shouldThrow !== false) {
        throw error;
      }

      return {
        getText: () => accumulatedText,
        getReasoning: () => accumulatedReasoning || undefined,
        usage,
      };
    }
  }

  /**
   * å¸¦ Trace çš„ Completions
   */
  public async completionsForTrace(
    params: CompletionsParams,
    options?: { signal?: AbortSignal; timeout?: number }
  ): Promise<CompletionsResult> {
    // ç®€åŒ–ç‰ˆï¼Œæš‚ä¸å®ç° trace
    return this.completions(params, options);
  }

  /**
   * è·å–æ¨¡å‹åˆ—è¡¨
   */
  public async models(): Promise<SdkModel[]> {
    await this.ensureInitialized();
    return this.apiClient.listModels();
  }

  /**
   * è·å– Embedding ç»´åº¦
   */
  public async getEmbeddingDimensions(model: Model): Promise<number> {
    await this.ensureInitialized();
    return this.apiClient.getEmbeddingDimensions(model);
  }

  /**
   * ç”Ÿæˆå›¾ç‰‡
   */
  public async generateImage(params: GenerateImageParams): Promise<string[]> {
    await this.ensureInitialized();
    return this.apiClient.generateImage(params as any);
  }

  /**
   * è·å– Base URL
   */
  public getBaseURL(): string {
    return this.provider.apiHost || '';
  }

  /**
   * è·å– API Key
   */
  public getApiKey(): string {
    return this.provider.apiKey || '';
  }

  // ==================== Private Methods ====================

  /**
   * è½¬æ¢æ¶ˆæ¯æ ¼å¼
   * ğŸ”§ æ”¯æŒ MCP æç¤ºè¯æ³¨å…¥æ¨¡å¼
   */
  private transformMessages(
    messages: Message[] | string,
    assistant: Assistant,
    mcpTools?: MCPTool[],
    mcpMode?: 'prompt' | 'function'
  ): Message[] {
    // å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºæ¶ˆæ¯æ•°ç»„
    if (typeof messages === 'string') {
      return [{ role: 'user', content: messages }];
    }

    // æ„å»ºç³»ç»Ÿæç¤ºè¯
    let systemPrompt = assistant.prompt || '';

    // ğŸ”§ å…³é”®ï¼šå¦‚æœæœ‰ MCP å·¥å…·ä¸”ä½¿ç”¨æç¤ºè¯æ³¨å…¥æ¨¡å¼ï¼Œæ³¨å…¥å·¥å…·å®šä¹‰
    console.log(`[AiProvider] MCP çŠ¶æ€ - å·¥å…·æ•°é‡: ${mcpTools?.length || 0}, æ¨¡å¼: ${mcpMode}`);
    
    if (mcpTools && mcpTools.length > 0 && mcpMode === 'prompt') {
      console.log(`[AiProvider] ğŸ”§ æç¤ºè¯æ³¨å…¥æ¨¡å¼ï¼šæ³¨å…¥ ${mcpTools.length} ä¸ª MCP å·¥å…·`);
      systemPrompt = buildSystemPrompt(systemPrompt, mcpTools as any);
      console.log(`[AiProvider] æ³¨å…¥åç³»ç»Ÿæç¤ºè¯é•¿åº¦: ${systemPrompt.length}`);
    } else if (mcpTools && mcpTools.length > 0) {
      console.log(`[AiProvider] å‡½æ•°è°ƒç”¨æ¨¡å¼ï¼š${mcpTools.length} ä¸ªå·¥å…·å°†é€šè¿‡ tools å‚æ•°ä¼ é€’`);
    }

    // æ·»åŠ ç³»ç»Ÿæç¤ºè¯
    const result: Message[] = [];
    if (systemPrompt) {
      result.push({ role: 'system', content: systemPrompt });
    }

    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    result.push(...messages);
    return result;
  }

  /**
   * å¤„ç†å•ä¸ª Chunk
   * è¿”å›è§£æå‡ºçš„æ–‡æœ¬ã€æ¨ç†å†…å®¹å’Œä½¿ç”¨ç»Ÿè®¡
   */
  private processChunk(
    rawChunk: any,
    onChunk?: (chunk: Chunk) => void
  ): { text?: string; reasoning?: string; usage?: CompletionsResult['usage'] } {
    const result: { text?: string; reasoning?: string; usage?: CompletionsResult['usage'] } = {};

    // === OpenAI å…¼å®¹æ ¼å¼ ===
    if (rawChunk.choices && rawChunk.choices.length > 0) {
      for (const choice of rawChunk.choices) {
        if (!choice) continue;

        // æ”¯æŒ deltaï¼ˆæµå¼ï¼‰å’Œ messageï¼ˆéæµå¼ï¼‰
        let contentSource: any = null;
        if (choice.delta && Object.keys(choice.delta).length > 0) {
          contentSource = choice.delta;
        } else if (choice.message) {
          contentSource = choice.message;
        }

        if (!contentSource) continue;

        // å¤„ç†æ¨ç†å†…å®¹
        const reasoningText = 
          contentSource.reasoning_content || 
          contentSource.reasoning || 
          contentSource.thinking?.content;
        if (reasoningText) {
          result.reasoning = reasoningText;
          if (onChunk) {
            onChunk({
              type: ChunkType.THINKING_DELTA,
              text: reasoningText,
            } as Chunk);
          }
        }

        // å¤„ç†æ–‡æœ¬å†…å®¹ï¼ˆæ”¯æŒ null å€¼ï¼‰
        if (contentSource.content !== undefined && contentSource.content !== null) {
          result.text = contentSource.content;
          if (onChunk) {
            onChunk({
              type: ChunkType.TEXT_DELTA,
              text: contentSource.content,
            } as Chunk);
          }
        }
      }

      // å¤„ç† usage
      if (rawChunk.usage) {
        result.usage = {
          prompt_tokens: rawChunk.usage.prompt_tokens || 0,
          completion_tokens: rawChunk.usage.completion_tokens || 0,
          total_tokens: rawChunk.usage.total_tokens || 0,
        };
      }
    }
    // === Gemini æ ¼å¼ ===
    else if (rawChunk.candidates?.[0]?.content?.parts) {
      for (const part of rawChunk.candidates[0].content.parts) {
        if (part.thought && part.text) {
          result.reasoning = part.text;
          if (onChunk) {
            onChunk({
              type: ChunkType.THINKING_DELTA,
              text: part.text,
            } as Chunk);
          }
        } else if (part.text) {
          result.text = part.text;
          if (onChunk) {
            onChunk({
              type: ChunkType.TEXT_DELTA,
              text: part.text,
            } as Chunk);
          }
        }
      }
    }
    // === Anthropic æ ¼å¼ ===
    else if (rawChunk.type === 'content_block_delta') {
      if (rawChunk.delta?.type === 'text_delta' && rawChunk.delta?.text) {
        result.text = rawChunk.delta.text;
        if (onChunk) {
          onChunk({
            type: ChunkType.TEXT_DELTA,
            text: rawChunk.delta.text,
          } as Chunk);
        }
      } else if (rawChunk.delta?.type === 'thinking_delta' && rawChunk.delta?.thinking) {
        result.reasoning = rawChunk.delta.thinking;
        if (onChunk) {
          onChunk({
            type: ChunkType.THINKING_DELTA,
            text: rawChunk.delta.thinking,
          } as Chunk);
        }
      }
    }
    // === ç›´æ¥æ–‡æœ¬æ ¼å¼ ===
    else if (typeof rawChunk === 'string') {
      result.text = rawChunk;
      if (onChunk) {
        onChunk({
          type: ChunkType.TEXT_DELTA,
          text: rawChunk,
        } as Chunk);
      }
    }
    // === æœªçŸ¥æ ¼å¼å›é€€ ===
    else if (rawChunk.content || rawChunk.text || rawChunk.response) {
      const text = rawChunk.content || rawChunk.text || rawChunk.response;
      if (typeof text === 'string') {
        result.text = text;
        if (onChunk) {
          onChunk({
            type: ChunkType.TEXT_DELTA,
            text: text,
          } as Chunk);
        }
      }
    }

    return result;
  }
}

// ==================== Helper Functions ====================

/**
 * ä» Model åˆ›å»º Provider
 */
export function modelToProvider(model: Model): Provider {
  return {
    id: model.provider || 'custom',
    type: (model.providerType || model.provider || 'openai') as any,
    name: model.name || model.id,
    apiKey: model.apiKey || '',
    apiHost: model.baseUrl || '',
    models: [],
    enabled: true,
  };
}

/**
 * åˆ›å»º AiProvider å®ä¾‹ï¼ˆä» Modelï¼‰
 */
export function createAiProviderFromModel(model: Model): AiProvider {
  const provider = modelToProvider(model);
  return new AiProvider(provider);
}
