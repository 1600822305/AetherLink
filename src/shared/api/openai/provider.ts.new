/**
 * OpenAI Provider
 * 负责与OpenAI API通信
 */
import OpenAI from 'openai';
import { createClient } from './client';
import { streamCompletion } from './stream';
import {
  createToolsParams,
  ToolType
} from './tools';
import {
  supportsMultimodal,
  supportsWebSearch,
  getWebSearchParams
} from './client';
import { getMainTextContent } from '../../utils/messageUtils';
import type { BaseProvider } from '../baseProvider';
import type { Message, Model } from '../../types';

/**
 * 基础OpenAI Provider
 */
export abstract class BaseProvider implements BaseProvider {
  protected client: OpenAI;
  protected model: Model;

  constructor(model: Model) {
    this.model = model;
    this.client = createClient(model);
  }

  /**
   * 检查模型是否支持多模态
   * @param model 模型对象（可选）
   * @returns 是否支持多模态
   */
  protected supportsMultimodal(model?: Model): boolean {
    const actualModel = model || this.model;
    return supportsMultimodal(actualModel);
  }

  /**
   * 检查模型是否支持网页搜索
   */
  protected supportsWebSearch(): boolean {
    return supportsWebSearch(this.model);
  }

  /**
   * 检查模型是否支持推理优化
   */
  protected supportsReasoning(): boolean {
    return supportsReasoning(this.model);
  }

  /**
   * 获取温度参数
   */
  protected getTemperature(): number {
    return this.model.temperature || 1.0;
  }

  /**
   * 获取top_p参数
   */
  protected getTopP(): number {
    return this.model.top_p || 1.0;
  }

  /**
   * 获取推理优化参数
   * 根据模型类型和助手设置返回不同的推理参数
   * @param assistant 助手对象
   * @param model 模型对象
   * @returns 推理参数
   */
  protected getReasoningEffort(assistant?: any, model?: Model): any {
    const actualModel = model || this.model;

    // 导入必要的函数和常量
    const {
      isReasoningModel,
      isOpenAIReasoningModel,
      isClaudeReasoningModel,
      isGeminiReasoningModel,
      isQwenReasoningModel,
      isGrokReasoningModel
    } = require('../../config/models');

    const {
      EFFORT_RATIO,
      DEFAULT_MAX_TOKENS,
      findTokenLimit
    } = require('../../config/constants');

    // 如果模型不支持推理，返回空对象
    if (!isReasoningModel(actualModel)) {
      return {};
    }

    // 获取推理努力程度
    const reasoningEffort = assistant?.settings?.reasoning_effort;

    // 如果未设置推理努力程度，根据模型类型返回禁用推理的参数
    if (!reasoningEffort) {
      // Qwen模型
      if (isQwenReasoningModel(actualModel)) {
        return { enable_thinking: false };
      }

      // Claude模型
      if (isClaudeReasoningModel(actualModel)) {
        return { thinking: { type: 'disabled' } };
      }

      // Gemini模型
      if (isGeminiReasoningModel(actualModel)) {
        return { reasoning_effort: 'none' };
      }

      // 默认情况
      return {};
    }

    // 计算推理token预算
    const effortRatio = EFFORT_RATIO[reasoningEffort];
    const tokenLimit = findTokenLimit(actualModel.id);

    // 如果找不到token限制，使用默认值
    if (!tokenLimit) {
      return { reasoning_effort: reasoningEffort };
    }

    const budgetTokens = Math.floor(
      (tokenLimit.max - tokenLimit.min) * effortRatio + tokenLimit.min
    );

    // 根据模型类型返回不同的推理参数

    // OpenAI模型
    if (isOpenAIReasoningModel(actualModel)) {
      return {
        reasoning_effort: reasoningEffort
      };
    }

    // Qwen模型
    if (isQwenReasoningModel(actualModel)) {
      return {
        enable_thinking: true,
        thinking_budget: budgetTokens
      };
    }

    // Grok模型
    if (isGrokReasoningModel(actualModel)) {
      return {
        reasoning_effort: reasoningEffort
      };
    }

    // Gemini模型
    if (isGeminiReasoningModel(actualModel)) {
      return {
        reasoning_effort: reasoningEffort
      };
    }

    // Claude模型
    if (isClaudeReasoningModel(actualModel)) {
      const maxTokens = assistant?.settings?.maxTokens;
      return {
        thinking: {
          type: 'enabled',
          budget_tokens: Math.max(1024, Math.min(budgetTokens, (maxTokens || DEFAULT_MAX_TOKENS) * effortRatio))
        }
      };
    }

    // 默认情况
    return {};
  }

  /**
   * 获取消息参数
   * 支持多种类型的消息内容，包括文本、图像、文件等
   * 支持不同模型特定的消息格式
   * @param message 消息对象
   * @param model 模型对象（可选）
   * @returns 消息参数
   */
  protected async getMessageParam(message: Message, model?: Model): Promise<any> {
    const actualModel = model || this.model;
    const isVision = this.supportsMultimodal(actualModel);
    const content = getMainTextContent(message);

    // 导入必要的函数
    const { findImageBlocks } = require('../../utils/messageUtils');
    const { findFileBlocks, FileTypes, getFileTypeByExtension, readFileContent } = require('../../utils/fileUtils');
    const {
      isClaudeModel,
      isGeminiModel,
      isGemmaModel
    } = require('../../utils/modelUtils');

    // 获取图片和文件块
    const imageBlocks = findImageBlocks(message);
    const fileBlocks = findFileBlocks(message);

    // 处理系统消息的特殊情况
    if (message.role === 'system') {
      // Claude模型的系统消息需要特殊处理
      if (isClaudeModel(actualModel)) {
        return {
          role: 'system',
          content
        };
      }

      // Gemini模型不支持系统消息，需要转换为用户消息
      if (isGeminiModel(actualModel)) {
        return {
          role: 'user',
          content: `<system>\n${content}\n</system>`
        };
      }

      // Gemma模型的系统消息需要特殊处理
      if (isGemmaModel(actualModel)) {
        return {
          role: 'user',
          content: `<start_of_turn>system\n${content}\n<end_of_turn>`
        };
      }

      // 默认处理系统消息
      return {
        role: 'system',
        content
      };
    }

    // 如果没有特殊内容，返回简单的文本消息
    if (imageBlocks.length === 0 && fileBlocks.length === 0) {
      // 根据不同模型处理消息
      if (isClaudeModel(actualModel)) {
        return {
          role: message.role,
          content
        };
      }

      if (isGeminiModel(actualModel)) {
        return {
          role: message.role === 'assistant' ? 'model' : 'user',
          content
        };
      }

      // 默认处理
      return {
        role: message.role,
        content
      };
    }

    // 处理多模态内容
    if (isVision && (imageBlocks.length > 0 || fileBlocks.length > 0)) {
      // 根据不同模型处理多模态内容
      if (isClaudeModel(actualModel)) {
        // Claude模型的多模态格式
        const parts: any[] = [];

        // 添加文本内容
        if (content) {
          parts.push({
            type: 'text',
            text: content
          });
        }

        // 添加图片内容
        for (const block of imageBlocks) {
          if (block.url) {
            parts.push({
              type: 'image',
              source: {
                type: 'url',
                url: block.url
              }
            });
          } else if (block.base64Data) {
            parts.push({
              type: 'image',
              source: {
                type: 'base64',
                media_type: block.mimeType || 'image/jpeg',
                data: block.base64Data
              }
            });
          }
        }

        return {
          role: message.role,
          content: parts
        };
      }

      if (isGeminiModel(actualModel)) {
        // Gemini模型的多模态格式
        const parts: any[] = [];

        // 添加文本内容
        if (content) {
          parts.push({
            text: content
          });
        }

        // 添加图片内容
        for (const block of imageBlocks) {
          if (block.url) {
            parts.push({
              inline_data: {
                mime_type: block.mimeType || 'image/jpeg',
                data: block.url.startsWith('data:') ? block.url.split(',')[1] : '[URL_IMAGE]'
              }
            });
          } else if (block.base64Data) {
            parts.push({
              inline_data: {
                mime_type: block.mimeType || 'image/jpeg',
                data: block.base64Data
              }
            });
          }
        }

        return {
          role: message.role === 'assistant' ? 'model' : 'user',
          parts
        };
      }

      // OpenAI模型的多模态格式（默认）
      const parts: any[] = [];

      // 添加文本内容
      if (content) {
        parts.push({
          type: 'text',
          text: content
        });
      }

      // 添加图片内容
      for (const block of imageBlocks) {
        if (block.url) {
          parts.push({
            type: 'image_url',
            image_url: {
              url: block.url,
              detail: 'auto'
            }
          });
        } else if (block.base64Data) {
          parts.push({
            type: 'image_url',
            image_url: {
              url: `data:${block.mimeType || 'image/jpeg'};base64,${block.base64Data}`,
              detail: 'auto'
            }
          });
        }
      }

      // 添加文件内容（如果支持）
      for (const block of fileBlocks) {
        if (block.file) {
          const fileType = getFileTypeByExtension(block.file.name || block.file.origin_name || '');

          // 只处理文本和文档类型的文件
          if (fileType === FileTypes.TEXT || fileType === FileTypes.DOCUMENT) {
            try {
              const fileContent = await readFileContent(block.file);
              if (fileContent) {
                parts.push({
                  type: 'text',
                  text: `文件: ${block.file.name || block.file.origin_name || '未知文件'}\n\n${fileContent}`
                });
              }
            } catch (error) {
              console.error(`[OpenAIProvider.getMessageParam] 读取文件内容失败:`, error);
            }
          }
        }
      }

      return {
        role: message.role,
        content: parts
      };
    }

    // 处理包含文件但不支持多模态的情况
    if (fileBlocks.length > 0) {
      let combinedContent = content ? content + '\n\n' : '';

      // 添加文件内容
      for (const block of fileBlocks) {
        if (block.file) {
          const fileType = getFileTypeByExtension(block.file.name || block.file.origin_name || '');

          // 只处理文本和文档类型的文件
          if (fileType === FileTypes.TEXT || fileType === FileTypes.DOCUMENT) {
            try {
              const fileContent = await readFileContent(block.file);
              if (fileContent) {
                combinedContent += `文件: ${block.file.name || block.file.origin_name || '未知文件'}\n\n${fileContent}\n\n`;
              }
            } catch (error) {
              console.error(`[OpenAIProvider.getMessageParam] 读取文件内容失败:`, error);
            }
          }
        }
      }

      // 根据不同模型处理
      if (isGeminiModel(actualModel)) {
        return {
          role: message.role === 'assistant' ? 'model' : 'user',
          parts: [{ text: combinedContent.trim() }]
        };
      }

      // 默认处理
      return {
        role: message.role,
        content: combinedContent.trim()
      };
    }

    // 默认返回文本内容
    // 根据不同模型处理
    if (isGeminiModel(actualModel)) {
      return {
        role: message.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: content }]
      };
    }

    // 默认处理
    return {
      role: message.role,
      content
    };
  }

  /**
   * 构建系统提示
   */
  protected buildSystemPrompt(prompt: string, tools?: any[]): string {
    // 基本系统提示
    let systemPrompt = prompt || '';

    // 如果有工具，添加工具说明
    if (tools && tools.length > 0) {
      systemPrompt += '\n\n你有以下工具可用:\n';
      tools.forEach((tool, index) => {
        systemPrompt += `${index + 1}. ${tool.function.name}: ${tool.function.description}\n`;
      });
      systemPrompt += '\n请在适当的时候使用这些工具。';
    }

    return systemPrompt;
  }

  /**
   * 测试API连接
   */
  public async testConnection(): Promise<boolean> {
    try {
      const response = await this.client.chat.completions.create({
        model: this.model.id,
        messages: [{ role: 'user', content: 'Hello' }],
        max_tokens: 5,
      });
      return Boolean(response.choices[0].message);
    } catch (error) {
      console.error('API连接测试失败:', error);
      return false;
    }
  }

  /**
   * 抽象方法：发送聊天消息
   */
  public abstract sendChatMessage(
    messages: Message[],
    options?: {
      onUpdate?: (content: string, reasoning?: string) => void;
      enableWebSearch?: boolean;
      enableThinking?: boolean;
      tools?: ToolType[];
      systemPrompt?: string;
    }
  ): Promise<string>;
}

/**
 * OpenAI Provider实现类
 */
export class OpenAIProvider extends BaseProvider {
  constructor(model: Model) {
    super(model);
  }

  /**
   * 发送聊天消息
   * @param messages 消息数组
   * @param options 选项
   * @returns 响应内容
   */
  public async sendChatMessage(
    messages: Message[],
    options?: {
      onUpdate?: (content: string, reasoning?: string) => void;
      enableWebSearch?: boolean;
      enableThinking?: boolean;
      tools?: ToolType[];
      systemPrompt?: string;
    }
  ): Promise<string> {
    console.log(`[OpenAIProvider.sendChatMessage] 开始处理聊天请求, 模型: ${this.model.id}`);

    const {
      onUpdate,
      enableWebSearch = false,
      enableThinking = false,
      tools = [],
      systemPrompt = ''
    } = options || {};

    // 记录原始消息数量
    console.log(`[OpenAIProvider.sendChatMessage] 原始消息数量: ${messages.length}`);

    // 准备消息数组 - 使用最佳实例风格的消息处理
    const apiMessages = [];

    // 添加系统消息 - 与最佳实例保持一致，始终添加系统消息，即使提示词为空
    // 构建系统提示，包含工具说明
    const finalSystemPrompt = this.buildSystemPrompt(
      systemPrompt,
      tools.length > 0 ? createToolsParams(tools).tools : undefined
    );

    // 系统消息始终作为第一条消息
    apiMessages.push({
      role: 'system',
      content: finalSystemPrompt
    });

    console.log(`[OpenAIProvider.sendChatMessage] 添加系统提示: ${finalSystemPrompt.substring(0, 50)}${finalSystemPrompt.length > 50 ? '...' : ''}`);


    // 按创建时间排序消息，确保顺序正确
    const sortedMessages = [...messages].sort((a, b) => {
      const timeA = new Date(a.createdAt).getTime();
      const timeB = new Date(b.createdAt).getTime();
      return timeA - timeB; // 升序排列，最早的在前面
    });

    console.log(`[OpenAIProvider.sendChatMessage] 消息已按时间排序，总数: ${sortedMessages.length}`);

    // 添加用户和助手消息 - 保持原始角色
    for (const message of sortedMessages) {
      try {
        // 使用增强的getMessageParam方法获取消息参数
        const messageParam = await this.getMessageParam(message);

        // 检查消息内容是否为空
        const hasContent = typeof messageParam.content === 'string'
          ? messageParam.content.trim().length > 0
          : Array.isArray(messageParam.content) && messageParam.content.length > 0;

        // 只添加有内容的消息
        if (hasContent) {
          // 检查是否已经有相同角色的连续消息
          const lastMessage = apiMessages[apiMessages.length - 1];
          if (lastMessage && lastMessage.role === messageParam.role && messageParam.role !== 'system') {
            console.log(`[OpenAIProvider.sendChatMessage] 跳过连续的${messageParam.role}消息，避免角色重复`);
            continue;
          }

          // 添加消息参数
          apiMessages.push(messageParam);

          // 记录日志
          const contentSummary = typeof messageParam.content === 'string'
            ? messageParam.content.substring(0, 50) + (messageParam.content.length > 50 ? '...' : '')
            : '[复杂内容]';

          console.log(`[OpenAIProvider.sendChatMessage] 添加消息: role=${messageParam.role}, content=${contentSummary}`);
        }
      } catch (error) {
        console.error(`[OpenAIProvider.sendChatMessage] 处理消息失败:`, error);

        // 降级处理：使用简单的文本内容
        const content = this.getMessageContent(message);
        if (content.trim()) {
          apiMessages.push({
            role: message.role,
            content: content
          });

          console.log(`[OpenAIProvider.sendChatMessage] 降级添加消息: role=${message.role}, content=${content.substring(0, 50)}${content.length > 50 ? '...' : ''}`);
        }
      }
    }

    // 确保系统消息始终在第一位
    const systemMessageIndex = apiMessages.findIndex(msg => msg.role === 'system');
    if (systemMessageIndex > 0) {
      const systemMessage = apiMessages.splice(systemMessageIndex, 1)[0];
      apiMessages.unshift(systemMessage);
      console.log(`[OpenAIProvider.sendChatMessage] 将系统消息移到第一位`);
    }

    console.log(`[OpenAIProvider.sendChatMessage] 最终API消息数量: ${apiMessages.length}`);

    // 确保至少有一条用户消息 - 最佳实例风格的安全检查
    if (apiMessages.length === 0 || !apiMessages.some(msg => msg.role === 'user')) {
      console.warn('[OpenAIProvider.sendChatMessage] 警告: 消息列表中没有用户消息，添加默认用户消息');

      // 添加一个默认的用户消息
      apiMessages.push({
        role: 'user',
        content: '你好'
      });

      console.log('[OpenAIProvider.sendChatMessage] 添加默认用户消息: 你好');
    }

    // 强制检查：确保messages数组不为空
    if (apiMessages.length === 0) {
      console.error('[OpenAIProvider.sendChatMessage] 严重错误: 消息数组为空，添加默认消息');

      // 添加一个默认的用户消息
      apiMessages.push({
        role: 'user',
        content: '你好'
      });

      console.log('[OpenAIProvider.sendChatMessage] 添加默认用户消息: 你好');
    }

    // 记录最终消息数组
    console.log(`[OpenAIProvider.sendChatMessage] 最终消息数组:`, JSON.stringify(apiMessages.map(m => ({
      role: m.role,
      content: typeof m.content === 'string'
        ? (m.content.substring(0, 30) + (m.content.length > 30 ? '...' : ''))
        : '[复杂内容]'
    }))));

    // 详细记录每条消息的角色和内容前30个字符，便于调试
    console.log(`[OpenAIProvider.sendChatMessage] 消息详情:`);
    apiMessages.forEach((msg, index) => {
      console.log(`  [${index}] ${msg.role}: ${
        typeof msg.content === 'string'
          ? (msg.content.substring(0, 30) + (msg.content.length > 30 ? '...' : ''))
          : '[复杂内容]'
      }`);
    });

    // 构建请求参数 - 与最佳实例保持一致，始终启用流式输出
    const requestParams: any = {
      model: this.model.id,
      messages: apiMessages,
      temperature: this.getTemperature(),
      top_p: this.getTopP(),
      max_tokens: this.model.maxTokens,
      stream: true // 始终启用流式输出，与最佳实例保持一致
    };

    console.log(`[OpenAIProvider.sendChatMessage] 请求参数:`, {
      model: this.model.id,
      messagesCount: apiMessages.length,
      temperature: requestParams.temperature,
      top_p: requestParams.top_p,
      max_tokens: requestParams.max_tokens,
      stream: requestParams.stream // 添加流式输出信息
    });

    // 检查API密钥和基础URL是否设置
    if (!this.model.apiKey) {
      console.error('[OpenAIProvider.sendChatMessage] 错误: API密钥未设置');
      throw new Error('API密钥未设置，请在设置中配置OpenAI API密钥');
    }

    if (!this.model.baseUrl) {
      console.warn('[OpenAIProvider.sendChatMessage] 警告: 基础URL未设置，使用默认值');
    }

    // 添加网页搜索参数
    if (enableWebSearch && this.supportsWebSearch()) {
      Object.assign(requestParams, getWebSearchParams(this.model, enableWebSearch));
      console.log(`[OpenAIProvider.sendChatMessage] 启用网页搜索功能`);
    }

    // 处理工具参数
    const allTools = [...tools];
    if (enableThinking) {
      allTools.push(ToolType.THINKING);
      console.log(`[OpenAIProvider.sendChatMessage] 启用思考工具`);
    }

    if (allTools.length > 0) {
      const toolParams = createToolsParams(allTools);
      requestParams.tools = toolParams.tools;
      requestParams.tool_choice = toolParams.tool_choice;
      console.log(`[OpenAIProvider.sendChatMessage] 配置工具参数: ${allTools.join(', ')}`);

      // 创建通用工具列表，用于工具调用处理
      const { openAIToolToTool } = require('./tools');
      const genericTools = toolParams.tools.map((tool: any) => openAIToolToTool([tool], { function: { name: tool.function.name } }));

      // 将通用工具列表存储在请求参数中，以便在处理响应时使用
      (requestParams as any).genericTools = genericTools;
      console.log(`[OpenAIProvider.sendChatMessage] 创建通用工具列表: ${genericTools.length}个工具`);
    }

    // 添加推理参数
    if (this.supportsReasoning()) {
      const reasoningParams = this.getReasoningEffort();
      Object.assign(requestParams, reasoningParams);
      console.log(`[OpenAIProvider.sendChatMessage] 添加推理参数:`, reasoningParams);
    }

    try {
      // 使用流式响应处理
      if (onUpdate) {
        console.log(`[OpenAIProvider.sendChatMessage] 使用流式响应模式（有回调）`);
        return await this.handleStreamResponse(requestParams, onUpdate);
      } else {
        // 即使没有回调，也使用流式响应，但结果会在完成后一次性返回
        // 这与最佳实例的行为一致，最佳实例总是使用流式响应
        console.log(`[OpenAIProvider.sendChatMessage] 使用流式响应模式（无回调）`);
        return await this.handleStreamResponseWithoutCallback(requestParams);
      }
    } catch (error) {
      console.error('[OpenAIProvider.sendChatMessage] API请求失败:', error);
      throw error;
    }
  }

  /**
   * 获取消息内容
   * @param message 消息对象
   * @returns 消息内容
   */
  protected getMessageContent(message: Message): string {
    return getMainTextContent(message);
  }

  /**
   * 处理流式响应
   * @param params 请求参数
   * @param onUpdate 更新回调
   * @returns 响应内容
   */
  private async handleStreamResponse(
    params: any,
    onUpdate: (content: string, reasoning?: string) => void
  ): Promise<string> {
    // 创建工具响应数组，用于存储工具调用响应
    const toolResponses: any[] = [];

    // 创建增强的回调函数，处理工具调用
    const enhancedCallback = async (content: string, reasoning?: string) => {
      // 调用原始回调函数
      onUpdate(content, reasoning);

      // 如果有通用工具列表，尝试解析工具调用
      if (params.genericTools && params.genericTools.length > 0) {
        try {
          // 导入工具调用处理函数
          const { parseAndCallTools } = require('../tools/parseAndCallTools');

          // 解析并调用工具
          await parseAndCallTools(
            content,
            toolResponses,
            onUpdate,
            undefined,
            this.model,
            params.genericTools
          );
        } catch (error) {
          console.error('[OpenAIProvider.handleStreamResponse] 处理工具调用失败:', error);
        }
      }
    };

    // 调用流式完成函数
    return await streamCompletion(
      this.client,
      this.model.id,
      params.messages,
      params.temperature,
      params.max_tokens || params.max_completion_tokens,
      enhancedCallback,
      {
        ...params,
        enableReasoning: this.supportsReasoning()
      }
    );
  }

  /**
   * 处理流式响应（无回调）
   * 使用流式响应但不使用回调，结果会在完成后一次性返回
   * 这与最佳实例的行为一致
   * @param params 请求参数
   * @returns 响应内容
   */
  private async handleStreamResponseWithoutCallback(params: any): Promise<string> {
    try {
      console.log('[OpenAIProvider.handleStreamResponseWithoutCallback] 开始处理流式响应（无回调）');

      // 创建工具响应数组，用于存储工具调用响应
      const toolResponses: any[] = [];

      // 创建一个虚拟回调函数，用于处理流式响应
      let fullResponse = '';
      let lastUpdateTime = Date.now();
      const updateInterval = 50; // 50毫秒更新一次，避免过于频繁的更新

      // 创建一个虚拟回调函数
      const virtualCallback = async (content: string) => {
        // 只在内容有变化且距离上次更新超过指定时间间隔时才触发回调
        if (content !== fullResponse && (Date.now() - lastUpdateTime) > updateInterval) {
          // 更新完整响应
          fullResponse = content;

          // 更新最后更新时间
          lastUpdateTime = Date.now();

          // 这里我们可以添加其他处理逻辑，例如更新UI
          console.log(`[OpenAIProvider.virtualCallback] 更新内容，当前长度: ${content.length}`);

          // 如果有通用工具列表，尝试解析工具调用
          if (params.genericTools && params.genericTools.length > 0) {
            try {
              // 导入工具调用处理函数
              const { parseAndCallTools } = require('../tools/parseAndCallTools');

              // 解析并调用工具
              await parseAndCallTools(
                content,
                toolResponses,
                undefined,
                undefined,
                this.model,
                params.genericTools
              );
            } catch (error) {
              console.error('[OpenAIProvider.virtualCallback] 处理工具调用失败:', error);
            }
          }
        }
      };

      // 使用streamCompletion函数处理流式响应
      return await streamCompletion(
        this.client,
        this.model.id,
        params.messages,
        params.temperature,
        params.max_tokens || params.max_completion_tokens,
        virtualCallback,
        {
          ...params,
          enableReasoning: this.supportsReasoning()
        }
      );
    } catch (error) {
      console.error('OpenAI API流式请求失败:', error);
      // 不使用logApiError，直接记录错误
      console.error('错误详情:', error);
      throw error;
    }
  }
}
